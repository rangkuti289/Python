# -*- coding: utf-8 -*-
"""TA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JiGo2L_XthbUNjiJMb2cUtV95GkYVJWA
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries
import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import matplotlib.ticker as ticker
from sklearn import preprocessing
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

test_ta = pd.read_excel("drive/MyDrive/TA.xlsx")

# input data yang di inginkan
# from google.colab import files
# uploaded = files.upload()

#untuk mendeklarasikan datatb
#import io
#test_ta = pd.read_excel(io.BytesIO(uploaded['data_test.xlsx']))
#test_ta [0:5]

test_ta.axes

test_ta [0:5]

#Menghapus Kolom yang tidak digunakan
data = test_ta.drop(['status_bangunan','status_lahan','ada_tabung_gas','ada_lemari_es','ada_ac','ada_pemanas','ada_telepon','ada_tv','ada_emas','ada_laptop','ada_sepeda','ada_motor','ada_mobil','ada_perahu','ada_motor_tempel','ada_perahu_motor','ada_kapal','aset_tak_bergerak','luas_atb','rumah_lain','jumlah_sapi','jumlah_kerbau','jumlah_kuda','jumlah_babi','jumlah_kambing','sta_art_usaha','JnsKelamin','TmpLahir','TglLahir','Hub_KRT','NUK', 'lapangan_usahaart','omset_usaha','lokasi_usaha','Lapangan_usaha','kepemilikan_kartu','Status_pekerjaan','NMPROP','NMKAB','Hubkel'], axis = 1)

data [0:5]

data.shape

data.isnull().sum()

#Menghapus baris data yang kosong
data.dropna(inplace=True)
data.isnull().sum()

data.shape

#menentukan variabel independen mempengaruhi
data1 = data[['Jumlah_ART', 'Jumlah_Keluarga', 'Umur', 'jenis_disabilitas','Partisipasi_sekolah','Pendidikan_tertinggi','Sta_Bekerja','ada_PKH','ada_KKS']]

data1.head()

import numpy as np
data1.describe() # Untuk fitur numerik (atau continous)
data1.describe(exclude = np.number) # Untuk fitur kategorik

#Membuat Labelling
label = {
    'Tidak/belum pernah sekolah':1,
    'Tidak bersekolah lagi':2,
    'Masih sekolah':3
}

data1['Partisipasi_sekolah'].replace(label, inplace=True)

#Cek Unique Values
data["Sta_Bekerja"].unique()

#Membuat Labeling
label = {
    'YA':1,
    'TIDAK':0
}
data1['Sta_Bekerja'].replace(label, inplace =True)

#Membuat Labelling
label = {
    'Tidak cacat':0, 
    'Tuna netra & cacat tubuh':1, 
    'Tuna rungu':1,
       'Tuna rungu, wicara, netra & cacat tubuh':1,
        'Cacat fisik & mental':1,
       'Tuna daksa/cacat tubuh':1,
        'Tuna rungu, wicara & cacat tubuh':1,
       'Cacat mental retardasi':1,
        'Tuna netra/buta':1,
       'Mantan penderita gangguan jiwa':0,
        'Tuna wicara':1,
       'Tuna rungu & wicara':1,
        'Tuna netra, rungu & wicara':1
}

data1['jenis_disabilitas'].replace(label, inplace=True)

#Membuat Labelling
label = {
    'SD/SLB':1,
    'Paket A':1,
    'M. Ibtidaiyah':1,
    'SMP/SMPLB':2,
    'Paket B':2,
    'M. Tsanawiyah':2,
    'M. Aliyah':3,
    'SMA/SMK/SMALB':3,
    'Paket C':3,
    'Perguruan Tinggi':4

}

data1['Pendidikan_tertinggi'].replace(label, inplace=True)

# #Mengubah set data diskrit menjadi dummy
# data1 = pd.get_dummies(data)

data1 [0:5]

#variabel Dependen
y = data['ada_kip']
y [0:5]

# data1.astype(int) [0:5]

from sklearn.model_selection import train_test_split
data1_train, data1_test, y_train, y_test = train_test_split( data1, y, test_size=0.2, random_state=42)
print ('Train set:', data1_train.shape,  y_train.shape)
print ('Test set:', data1_test.shape,  y_test.shape)

#mengubah skala data menggunakan featuring scalling agar data seragam menggunakan sandar scaller
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(data1_train)

data1_train = scaler.transform(data1_train)
data1_test = scaler.transform(data1_test)

from sklearn.neighbors import KNeighborsClassifier

error = []

for i in range(1, 31):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(data1_train, y_train)

    pred_i = knn.predict(data1_test)
    error.append(np.mean(pred_i != y_test))

plt.figure()
plt.plot(range(1, 31), error, color='red', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Rata-Rata Error terhadap nilai K')
plt.xlabel('Nilai K')
plt.ylabel('Rata-Rata Error')
plt.show()

#Mengklasifikasikan dan mengimport KNN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(data1_train, y_train)

#prediksi
yhat = knn.predict(data1_test)
yhat [0:50]

#menghitung tingkat akurasi data prediksi
from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, knn.predict(data1_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
print(classification_report(y_test, yhat))

"""Validasi"""

#Cross Validation
from sklearn.model_selection import cross_val_score
import numpy as np
# use the same model as before
knn = KNeighborsClassifier(n_neighbors = 3)
scores = cross_val_score(knn, data1, y, cv=10, scoring='accuracy')
# print all 5 times scores 
print(scores)
# then I will do the average about these five scores to get more accuracy score.
print(scores.mean())

# Commented out IPython magic to ensure Python compatibility.
#Tunning Hyperparameter
import matplotlib.pyplot as plt 
# %matplotlib inline
# choose k between 1 to 31
k_range = range(1, 31)
k_scores = []
# use iteration to caclulator different k in models, then return the average accuracy based on the cross validation
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, data1, y, cv=10, scoring='accuracy')
    k_scores.append(scores.mean())
# plot to see clearly
plt.plot(k_range, k_scores)
plt.xlabel('Value of K for K-NN')
plt.ylabel('Cross-Validation Accuracy')
plt.show()
print(k_scores)

from sklearn.model_selection import RepeatedStratifiedKFold

cv_method = RepeatedStratifiedKFold(n_splits=5, 
                                    n_repeats=3, 
                                    random_state=999)

from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import PowerTransformer
params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}

gs_NB = GridSearchCV(estimator=modelnb, 
                     param_grid=params_NB, 
                     cv=cv_method,
                     verbose=1, 
                     scoring='accuracy')

Data_transformed = PowerTransformer().fit_transform(data1_test)

gs_NB.fit(Data_transformed, y_test);

gs_NB.best_params_

gs_NB.best_score_

results_NB = pd.DataFrame(gs_NB.cv_results_['params'])
results_NB['test_score'] = gs_NB.cv_results_['mean_test_score']
plt.plot(results_NB['var_smoothing'], results_NB['test_score'], marker = '.')    
plt.xlabel('Var. Smoothing')
plt.ylabel("Mean CV Score")
plt.title("NB Performance Comparison")
plt.show()

# predict the target on the test dataset
predict_test = gs_NB.predict(Data_transformed)

# Accuracy Score on test dataset
accuracy_test = accuracy_score(y_test,predict_test)
print('accuracy_score on test dataset : ', accuracy_test)

"""**EVALUASI**"""

print(confusion_matrix(y_test, yhat))
cm = confusion_matrix(y_test, yhat)
cm_display = ConfusionMatrixDisplay(cm).plot()
print('\nTrue Positives(TP) = ', cm[0,0])

print('\nTrue Negatives(TN) = ', cm[1,1])

print('\nFalse Positives(FP) = ', cm[0,1])

print('\nFalse Negatives(FN) = ', cm[1,0])

"""**Naive Bayes**"""

#untuk mengaktifkn klasifikasi naive bayes
from sklearn.naive_bayes import GaussianNB
modelnb = GaussianNB()
#memasukkan data training pada fungsi klasifikasi Naive Bayes.
nbtrain = modelnb.fit(data1_train, y_train)
nbtrain.class_count_

# menentukan hasil prediksi data testing, dapat menggunakan script
y_pred  = nbtrain.predict(data1_test)
y_pred [0:50]

from sklearn.metrics import accuracy_score

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

#nilai akurasi
from sklearn.metrics import classification_report 
print (classification_report(y_test, y_pred))

"""**Validasi**"""

#Cross Validation
from sklearn.model_selection import cross_val_score
import numpy as np
# use the same model as before
modelnb = GaussianNB()
scores = cross_val_score(modelnb, data1_train, y_train, cv=10, scoring='accuracy')
# print all 5 times scores 
print(scores)
# then I will do the average about these five scores to get more accuracy score.
print(scores.mean())

"""Evaluasi"""

#membuat confussion matriks
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)

from sklearn.metrics import ConfusionMatrixDisplay
print(confusion_matrix(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
cm_display = ConfusionMatrixDisplay(cm).plot()
print('\nTrue Positives(TP) = ', cm[0,0])

print('\nTrue Negatives(TN) = ', cm[1,1])

print('\nFalse Positives(FP) = ', cm[0,1])

print('\nFalse Negatives(FN) = ', cm[1,0])